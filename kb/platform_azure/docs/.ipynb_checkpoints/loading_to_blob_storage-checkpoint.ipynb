{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading to Blob Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Resources\n",
    "\n",
    "* https://docs.microsoft.com/en-us/azure/storage/blobs/storage-quickstart-blobs-python\n",
    "\n",
    "\n",
    "## Naming\n",
    "\n",
    "Azure blob storage has the same concepts as the AWS S3 service but different naming convetion, so find below 1:1 mappings AWS to Azure.\n",
    "\n",
    "* S3 to Blob storage\n",
    "* Bucket to Container\n",
    "* Key to Blob Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient, __version__\n",
    "\n",
    "\n",
    "try:\n",
    "    print(\"Azure Blob Storage v\" + __version__ + \" - Python quickstart sample\")\n",
    "\n",
    "    # Quick start code goes here\n",
    "\n",
    "except Exception as ex:\n",
    "    print('Exception:')\n",
    "    print(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Enviroment Variables\n",
    "\n",
    "### On Windows\n",
    "\n",
    "```powershell\n",
    "setx AZURE_STORAGE_CONNECTION_STRING \"<yourconnectionstring>\"\n",
    "```\n",
    "\n",
    "You can fetch the connection string by going to the Azure storage resource, settings, type \"connection\" and copy the value under key1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import pandas\n",
    "\n",
    "connect_str = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connect_str)\n",
    "container_name = \"testing\"\n",
    "\n",
    "# Create a local directory to hold blob data\n",
    "local_path = \"./data\"\n",
    "try:\n",
    "    os.mkdir(local_path)\n",
    "except FileExistsError:\n",
    "    shutil.rmtree(local_path)\n",
    "    os.mkdir(local_path)\n",
    "\n",
    "# Create a file in the local data directory to upload and download\n",
    "local_file_name = \"alessio\" + \".txt\"\n",
    "upload_file_path = os.path.join(local_path, local_file_name)\n",
    "\n",
    "# Write text to the file\n",
    "file = open(upload_file_path, 'w')\n",
    "file.write(\"Hello, World!\")\n",
    "file.close()\n",
    "\n",
    "# Create a blob client with blob name inside blob virtual folder \"/test\"\n",
    "blob_name = \"/test\" + local_file_name\n",
    "blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "\n",
    "print(\"\\nUploading to Azure Storage as blob:\\n\\t\" + local_file_name)\n",
    "\n",
    "# Upload the created file\n",
    "with open(upload_file_path, \"rb\") as data:\n",
    "    blob_client.upload_blob(data, overwrite=True)\n",
    "\n",
    "# Now Loading a Parquet File data.parquet\n",
    "print(\"Uploading the pandas parquet file\")\n",
    "# Creating the file\n",
    "data = {\"country\":[\"italy\", \"germany\", \"spain\"], \"sales\":[100, 40, 20]}\n",
    "df = pandas.DataFrame(data)\n",
    "df.to_parquet(\"data/data.parquet\")\n",
    "# We need a new client as we are using a new blob_name\n",
    "blob_name = \"/test/\" + \"data.parquet\"\n",
    "blob_client = blob_service_client.get_blob_client(container=container_name, blob=blob_name)\n",
    "\n",
    "with open(\"data/data.parquet\", \"rb\") as data:\n",
    "    blob_client.upload_blob(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "metadata": {
   "interpreter": {
    "hash": "8afddae3e2ea78f4f161dd72449e770da5b359952f8a30a4201267a9db7ead29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
